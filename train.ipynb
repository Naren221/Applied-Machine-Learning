{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data splits\n",
    "train = pd.read_csv('train.csv')\n",
    "validation = pd.read_csv('validation.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "train = train.dropna()\n",
    "validation = validation.dropna()\n",
    "test = test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     3472\n",
       "spam     537\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing the text data using TF-IDF\n",
    "def vectorize_data(train_data, validation_data, test_data):\n",
    "    \"\"\"Vectorize the text data using TF-IDF.\"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "    X_train = vectorizer.fit_transform(train_data['message'])\n",
    "    X_validation = vectorizer.transform(validation_data['message'])\n",
    "    X_test = vectorizer.transform(test_data['message'])\n",
    "    return X_train, X_validation, X_test, vectorizer\n",
    "\n",
    "X_train, X_validation, X_test, vectorizer = vectorize_data(train, validation, test)\n",
    "\n",
    "# Model training function\n",
    "def fit_model(X_train, y_train, model):\n",
    "    \"\"\"Train a model on the training data.\"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# model evaluation function\n",
    "def evaluate_model(model, X, y, average='binary'):\n",
    "    \"\"\"Evaluate the model and return precision, recall, and accuracy.\"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    recall = recall_score(y, y_pred, pos_label='spam', average=average)\n",
    "    precision = precision_score(y, y_pred, pos_label='spam', average=average)\n",
    "    return precision, recall, accuracy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Merge train and validation datasets for hyperparameter tuning\n",
    "def merge_train_validation(train_data, validation_data):\n",
    "    \"\"\"Merge train and validation datasets for hyperparameter tuning.\"\"\"\n",
    "    combined_data = pd.concat([train_data, validation_data])\n",
    "    return combined_data\n",
    "\n",
    "# Fit, tune, and evaluate model with proper train-validation split\n",
    "def fit_and_evaluate(train_data, validation_data, X_train, X_validation, model, param_grid):\n",
    "    \n",
    "    # Merge train and validation datasets\n",
    "    full_train_data = merge_train_validation(train_data, validation_data)\n",
    "    X_full_train = vectorizer.transform(full_train_data['message'])\n",
    "    y_full_train = full_train_data['label']\n",
    "    \n",
    "    # Hyperparameter tuning using full train+validation data\n",
    "    grid_search = GridSearchCV(model, param_grid, scoring='recall', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_full_train, y_full_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluate on original validation set\n",
    "    val_precision, val_recall, val_accuracy = evaluate_model(best_model, X_validation, validation_data['label'])\n",
    "    print(f\"Validation Data - Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "\n",
    "# Model benchmarking with hyperparameter tuning\n",
    "def benchmark_models(X_train, y_train, X_test, y_test, average='binary'):\n",
    "    \"\"\"Benchmark multiple models with hyperparameter tuning and display evaluation metrics.\"\"\"\n",
    "    models = {\n",
    "        'Naive Bayes': (MultinomialNB(), {'alpha': [0.01, 0.1, 1, 10]}),\n",
    "        'Logistic Regression': (LogisticRegression(), {}),\n",
    "        'SVM': (SVC(), {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']})\n",
    "    }\n",
    "    \n",
    "    best_model = None\n",
    "    best_model_name = None\n",
    "    best_recall = 0\n",
    "    \n",
    "    for model_name, (model, param_grid) in models.items():\n",
    "        print(f\"Tuning {model_name}...\")\n",
    "        \n",
    "        grid_search = GridSearchCV(model, param_grid, scoring='recall', cv=5, n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_model_candidate = grid_search.best_estimator_\n",
    "        \n",
    "        # Evaluate on test data\n",
    "        precision, recall, accuracy = evaluate_model(best_model_candidate, X_test, y_test, average=average)\n",
    "        print(f\"{model_name} - Precision: {precision:.4f}, Recall: {recall:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        # Track the best model based on recall\n",
    "        if recall > best_recall:\n",
    "            best_recall = recall\n",
    "            best_model = best_model_candidate\n",
    "            best_model_name = model_name\n",
    "    \n",
    "    print(f\"\\nBest Model: {best_model_name} with Recall: {best_recall:.4f}\")\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data - Precision: 0.9930, Recall: 0.7862, Accuracy: 0.9706\n",
      "Validation Data - Precision: 0.9787, Recall: 0.7667, Accuracy: 0.9664\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "X_train, X_validation, X_test, vectorizer = vectorize_data(train, validation, test)\n",
    "\n",
    "# Fit and evaluate a model with hyperparameter tuning\n",
    "logistic_regression_model = LogisticRegression()\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10], 'solver': ['liblinear']}\n",
    "best_model = fit_and_evaluate(train, validation, X_train, X_validation, logistic_regression_model, param_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Naive Bayes...\n",
      "Naive Bayes - Precision: 0.9521, Recall: 0.8825, Accuracy: 0.9783\n",
      "\n",
      "Tuning Logistic Regression...\n",
      "Logistic Regression - Precision: 0.9905, Recall: 0.7702, Accuracy: 0.9690\n",
      "\n",
      "Tuning SVM...\n",
      "SVM - Precision: 0.9357, Recall: 0.5608, Accuracy: 0.9432\n",
      "\n",
      "Best Model: Naive Bayes with Recall: 0.8825\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Benchmark multiple models\n",
    "best_model = benchmark_models(X_train, train['label'], X_test, test['label'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
